{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400000, 2]) torch.Size([1, 200000, 2]) torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch, numpy as np, matplotlib.pyplot as plt\n",
    "from src.TorchSimulation.receiver import BER\n",
    "from src.TorchDSP.loss import Qsq\n",
    "from src.TorchSimulation.utils import show_symb\n",
    "from src.TorchDSP.dataloader import MyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from src.JaxSimulation.dsp import BPS, bps, ddpll, cpr, mimoaf, MetaMIMO\n",
    "import src.JaxSimulation.adaptive_filter as af, jax\n",
    "from src.JaxSimulation.core import MySignal, SigTime\n",
    "from src.JaxSimulation.MetaOptimizer import *\n",
    "\n",
    "import os \n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]='false'\n",
    "\n",
    "def get_grp(f, Nch, Rs, Pch, Nsymb, NF, SF, L=2000, tag=',method=frequency cut'):\n",
    "    for key in f.keys():\n",
    "        if f[key].attrs['Nch'] == Nch and f[key].attrs['Rs(GHz)'] == Rs and f[key].attrs['Pch(dBm)'] == Pch and f[key]['SymbTx'].shape[1] == Nsymb and f[key].attrs['NF(dB)'] == NF and f[key].attrs['freqspace(Hz)']/1e9 / f[key].attrs['Rs(GHz)'] == SF and f[key].attrs['distance(km)'] == L:\n",
    "            return f[key][f'Rx(sps=2,chid=0{tag})']\n",
    "        \n",
    "def get_signal(f, Nch, Rs, Pch, Nsymb, NF, SF, L=2000):\n",
    "    for key in f.keys():\n",
    "        if f[key].attrs['Nch'] == Nch and f[key].attrs['Rs(GHz)'] == Rs and f[key].attrs['Pch(dBm)'] == Pch and f[key]['SymbTx'].shape[1] == Nsymb and f[key].attrs['NF(dB)'] == NF and f[key].attrs['freqspace(Hz)']/1e9 / f[key].attrs['Rs(GHz)'] == SF and f[key].attrs['distance(km)'] == L:\n",
    "            return f[key]\n",
    "        \n",
    "\n",
    "def Q_path(Rx, Tx, Ntest=10000, stride=10000):\n",
    "    Q = []\n",
    "    for t in  np.arange(0, Rx.shape[-2] - Ntest, stride):\n",
    "        Q.append(np.mean(BER(torch.tensor(Rx[t:t+Ntest]), torch.tensor(Tx[t:t+Ntest]))['Qsq']))\n",
    "    return Q\n",
    "\n",
    "\n",
    "train_data = MyDataset('dataset_A800/train.h5', Nch=[21], Rs=[40], Pch=[0, 1, 2], Nmodes=2,\n",
    "                        window_size=400, strides=400-15, Nwindow=200, truncate=0,\n",
    "                        Tx_window=True, pre_transform='Rx_DBP16')\n",
    "train_loader = DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "\n",
    "\n",
    "test_data = MyDataset('dataset_A800/test.h5', Nch=[21], Rs=[40], Pch=[0], Nmodes=2,\n",
    "                        window_size=200000, strides=1, Nwindow=1, truncate=0,\n",
    "                        Tx_window=True, pre_transform='Rx_DBP16')\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "for Rx, Tx, info in test_loader:\n",
    "    print(Rx.shape, Tx.shape, info.shape)\n",
    "    break\n",
    "\n",
    "const = np.unique(Tx)\n",
    "\n",
    "signal = MySignal(val=Rx[0].numpy(), t=SigTime(0,0,2), Fs=0)\n",
    "truth = MySignal(val=Tx[0].numpy(), t=SigTime(0,0,1), Fs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MetaMIMO(taps=32, train=True, MetaOpt=MetaGRUOpt(hidden_dim=2))\n",
    "z, params = model.init_with_output(jax.random.PRNGKey(0), signal, truth, True)\n",
    "\n",
    "from src.JaxSimulation.dsp import  construct_update\n",
    "update_step = construct_update(model, optax.adam(1e-4), device='cpu', loss_type='MSE')\n",
    "opt_state = optax.adam(1e-3).init(params['params'])\n",
    "state_init = params['state']\n",
    "state = params['state']\n",
    "param = params['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/10 loss: 0.0279416\n",
      "Batch 1/10 loss: 0.0353071\n",
      "Batch 2/10 loss: 0.029496353\n",
      "Batch 3/10 loss: 0.029809441\n",
      "Batch 4/10 loss: 0.03249835\n",
      "Batch 5/10 loss: 0.03148366\n",
      "Batch 6/10 loss: 0.032804873\n",
      "Batch 7/10 loss: 0.03562747\n",
      "Batch 8/10 loss: 0.035662964\n",
      "Batch 9/10 loss: 0.029397689\n",
      "epoch 0 train loss: 0.032002951949834824\n",
      "Batch 0/10 loss: 0.033270903\n",
      "Batch 1/10 loss: 0.03084565\n",
      "Batch 2/10 loss: 0.03142063\n",
      "Batch 3/10 loss: 0.030986998\n",
      "Batch 4/10 loss: 0.028030064\n",
      "Batch 5/10 loss: 0.029269632\n",
      "Batch 6/10 loss: 0.034228045\n",
      "Batch 7/10 loss: 0.030301597\n",
      "Batch 8/10 loss: 0.029152734\n",
      "Batch 9/10 loss: 0.032346282\n",
      "epoch 1 train loss: 0.03098525106906891\n",
      "Batch 0/10 loss: 0.030316519\n",
      "Batch 1/10 loss: 0.033585325\n",
      "Batch 2/10 loss: 0.029688066\n",
      "Batch 3/10 loss: 0.029683867\n",
      "Batch 4/10 loss: 0.035278816\n",
      "Batch 5/10 loss: 0.04561062\n",
      "Batch 6/10 loss: 0.031227829\n",
      "Batch 7/10 loss: 0.028927732\n",
      "Batch 8/10 loss: 0.039922707\n",
      "Batch 9/10 loss: 0.029442677\n",
      "epoch 2 train loss: 0.033368416130542755\n",
      "Batch 0/10 loss: 0.029826716\n",
      "Batch 1/10 loss: 0.041971903\n",
      "Batch 2/10 loss: 0.0290658\n",
      "Batch 3/10 loss: 0.029118937\n",
      "Batch 4/10 loss: 0.031254534\n",
      "Batch 5/10 loss: 0.028055597\n",
      "Batch 6/10 loss: 0.030365564\n",
      "Batch 7/10 loss: 0.03392663\n",
      "Batch 8/10 loss: 0.028320594\n",
      "Batch 9/10 loss: 0.028701227\n",
      "epoch 3 train loss: 0.031060749664902687\n",
      "Batch 0/10 loss: 0.030334394\n",
      "Batch 1/10 loss: 0.03176159\n",
      "Batch 2/10 loss: 0.032467216\n",
      "Batch 3/10 loss: 0.030965326\n",
      "Batch 4/10 loss: 0.031198572\n",
      "Batch 5/10 loss: 0.031974025\n",
      "Batch 6/10 loss: 0.028412549\n",
      "Batch 7/10 loss: 0.03096466\n",
      "Batch 8/10 loss: 0.031891294\n",
      "Batch 9/10 loss: 0.03239127\n",
      "epoch 4 train loss: 0.031236091628670692\n",
      "Batch 0/10 loss: 0.02850372\n",
      "Batch 1/10 loss: 0.028878612\n",
      "Batch 2/10 loss: 0.032507684\n",
      "Batch 3/10 loss: 0.031447273\n",
      "Batch 4/10 loss: 0.036491472\n",
      "Batch 5/10 loss: 0.033040065\n",
      "Batch 6/10 loss: 0.031920712\n",
      "Batch 7/10 loss: 0.030922366\n",
      "Batch 8/10 loss: 0.03371681\n",
      "Batch 9/10 loss: 0.03245368\n",
      "epoch 5 train loss: 0.0319882407784462\n",
      "Batch 0/10 loss: 0.03429926\n",
      "Batch 1/10 loss: 0.028491817\n",
      "Batch 2/10 loss: 0.03145807\n",
      "Batch 3/10 loss: 0.030998275\n",
      "Batch 4/10 loss: 0.030534415\n",
      "Batch 5/10 loss: 0.029877214\n",
      "Batch 6/10 loss: 0.028717838\n",
      "Batch 7/10 loss: 0.032301415\n",
      "Batch 8/10 loss: 0.029241651\n",
      "Batch 9/10 loss: 0.03014099\n",
      "epoch 6 train loss: 0.030606094747781754\n",
      "Batch 0/10 loss: 0.029109167\n",
      "Batch 1/10 loss: 0.028965754\n",
      "Batch 2/10 loss: 0.027798923\n",
      "Batch 3/10 loss: 0.03109606\n",
      "Batch 4/10 loss: 0.02981106\n",
      "Batch 5/10 loss: 0.030152095\n",
      "Batch 6/10 loss: 0.0330625\n",
      "Batch 7/10 loss: 0.028884498\n",
      "Batch 8/10 loss: 0.038389802\n",
      "Batch 9/10 loss: 0.031692915\n",
      "epoch 7 train loss: 0.030896279960870743\n",
      "Batch 0/10 loss: 0.028268278\n",
      "Batch 1/10 loss: 0.031397108\n",
      "Batch 2/10 loss: 0.031608626\n",
      "Batch 3/10 loss: 0.030978\n",
      "Batch 4/10 loss: 0.0316934\n",
      "Batch 5/10 loss: 0.034892075\n",
      "Batch 6/10 loss: 0.03501972\n",
      "Batch 7/10 loss: 0.029947862\n",
      "Batch 8/10 loss: 0.02946902\n",
      "Batch 9/10 loss: 0.032406203\n",
      "epoch 8 train loss: 0.031568028032779694\n",
      "Batch 0/10 loss: 0.03232671\n",
      "Batch 1/10 loss: 0.03180998\n",
      "Batch 2/10 loss: 0.03086851\n",
      "Batch 3/10 loss: 0.031385925\n",
      "Batch 4/10 loss: 0.02878895\n",
      "Batch 5/10 loss: 0.031097926\n",
      "Batch 6/10 loss: 0.030848281\n",
      "Batch 7/10 loss: 0.033945836\n",
      "Batch 8/10 loss: 0.028761145\n",
      "Batch 9/10 loss: 0.033200216\n",
      "epoch 9 train loss: 0.031303346157073975\n"
     ]
    }
   ],
   "source": [
    "Ls = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    N = len(train_loader)\n",
    "    ls = []\n",
    "    for i,(Rx, Tx, info) in enumerate(train_loader):\n",
    "        sig_input = MySignal(val=Rx[0].numpy(), t=SigTime(0,0,2), Fs=0)\n",
    "        sig_output = MySignal(val=Tx[0].numpy(), t=SigTime(0,0,1), Fs=0)\n",
    "        param, state, opt_state,l = update_step(param, state, opt_state, sig_input, sig_output)\n",
    "        ls.append(l)\n",
    "        print(f'Batch {i}/{N} loss:',l)\n",
    "    Ls = Ls + ls\n",
    "    print(f'epoch {epoch} train loss: {np.mean(ls)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = MetaMIMO(taps=32, train=False, MetaOpt=MetaGRUOpt())\n",
    "\n",
    "from functools import partial\n",
    "@partial(jax.jit, backend='cpu')\n",
    "def apply_model(var, signal, truth):\n",
    "    return model_test.apply(var, signal, truth, True, mutable='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (2, 2) but got shape (2, 16) instead for parameter \"kernel\" in \"/scan(MetaOpt)/linear_in/layers_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m z, _ \u001b[38;5;241m=\u001b[39m \u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mstate_init\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mapply_model\u001b[0;34m(var, signal, truth)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jax\u001b[38;5;241m.\u001b[39mjit, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_model\u001b[39m(var, signal, truth):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/TorchFiber/src/JaxSimulation/dsp.py:1242\u001b[0m, in \u001b[0;36mMetaMIMO.__call__\u001b[0;34m(self, signal, truth, update_state)\u001b[0m\n\u001b[1;32m   1240\u001b[0m ScanCell \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mscan(MetaMIMOCell, variable_broadcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m,split_rngs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})(taps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaps, train\u001b[38;5;241m=\u001b[39mmake_schedule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain), dims\u001b[38;5;241m=\u001b[39mdims, grad_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_max, const\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst, MetaOpt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMetaOpt)\n\u001b[1;32m   1241\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetaState\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39m_:ScanCell\u001b[38;5;241m.\u001b[39minit_carry(),())\n\u001b[0;32m-> 1242\u001b[0m state_new, (af_weights, Delta) \u001b[38;5;241m=\u001b[39m \u001b[43mScanCell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1243\u001b[0m y \u001b[38;5;241m=\u001b[39m ScanCell\u001b[38;5;241m.\u001b[39mapply_fn(af_weights, x) \u001b[38;5;66;03m# 所有的 symbol 都过了之后才运用filter\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_state: state\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m state_new\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/fiber2024/lib/python3.9/site-packages/flax/core/axes_scan.py:148\u001b[0m, in \u001b[0;36mscan.<locals>.scan_fn\u001b[0;34m(broadcast_in, init, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m f_flat, out_tree \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mapi_util\u001b[38;5;241m.\u001b[39mflatten_fun_nokwargs(\n\u001b[1;32m    145\u001b[0m   lu\u001b[38;5;241m.\u001b[39mwrap_init(broadcast_body), in_tree\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    147\u001b[0m in_pvals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(pe\u001b[38;5;241m.\u001b[39mPartialVal\u001b[38;5;241m.\u001b[39munknown, in_avals))\n\u001b[0;32m--> 148\u001b[0m _, out_pvals, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_to_jaxpr_nounits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_pvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m out_flat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pv, const \u001b[38;5;129;01min\u001b[39;00m out_pvals:\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/fiber2024/lib/python3.9/site-packages/flax/core/axes_scan.py:120\u001b[0m, in \u001b[0;36mscan.<locals>.scan_fn.<locals>.body_fn\u001b[0;34m(c, xs, init_mode)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbody_fn\u001b[39m(c, xs, init_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    116\u001b[0m   \u001b[38;5;66;03m# inject constants\u001b[39;00m\n\u001b[1;32m    117\u001b[0m   xs \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m ax, arg, x: (arg \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m broadcast \u001b[38;5;28;01melse\u001b[39;00m x), in_axes, args, xs\n\u001b[1;32m    119\u001b[0m   )\n\u001b[0;32m--> 120\u001b[0m   broadcast_out, c, ys \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbroadcast_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m init_mode:\n\u001b[1;32m    123\u001b[0m     ys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[1;32m    124\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m ax, y: (y \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m broadcast \u001b[38;5;28;01melse\u001b[39;00m ()), out_axes, ys\n\u001b[1;32m    125\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/TorchFiber/src/JaxSimulation/dsp.py:1159\u001b[0m, in \u001b[0;36mMetaMIMOCell.__call__\u001b[0;34m(self, state, inp)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# Step 2: update hidden state (Fix learning rate)\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# hidden, updates = state['hidden'], jax.tree_map(lambda x,y: -x*y, grads, self.lr_init)\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \n\u001b[1;32m   1157\u001b[0m \u001b[38;5;66;03m# Step 2: update hidden state (Meta learning rate)\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m add_in \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 1159\u001b[0m hidden, updates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMetaOpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;66;03m# Step 3: update parameters\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m theta0 \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/TorchFiber/src/JaxSimulation/MetaOptimizer.py:235\u001b[0m, in \u001b[0;36mMetaGRUOpt.__call__\u001b[0;34m(self, opt_state, grads, params)\u001b[0m\n\u001b[1;32m    233\u001b[0m I \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mconcatenate([I0, add_info], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)         \u001b[38;5;66;03m# (N, Nmodes+1)\u001b[39;00m\n\u001b[1;32m    234\u001b[0m I \u001b[38;5;241m=\u001b[39m pre_transform(I)                                \u001b[38;5;66;03m# (N,1)\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m)\u001b[49m                               \u001b[38;5;66;03m# (N, hidden_dim)\u001b[39;00m\n\u001b[1;32m    236\u001b[0m opt_state, output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRNN(opt_state, I)          \u001b[38;5;66;03m# hidden: [(N, hidden_dim) ]xdepth  output: (N, hidden_dim)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m lr \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m-\u001b[39m complex_sigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_out(output)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_max   \u001b[38;5;66;03m# (N, 1)\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/fiber2024/lib/python3.9/site-packages/flax/linen/combinators.py:105\u001b[0m, in \u001b[0;36mSequential.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    103\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmpty Sequential module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mtuple\u001b[39m):\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/fiber2024/lib/python3.9/site-packages/flax/linen/linear.py:257\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;129m@compact\u001b[39m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    The transformed input.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m   kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    264\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[1;32m    265\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_init, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype\n\u001b[1;32m    266\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/fiber2024/lib/python3.9/site-packages/flax/core/scope.py:982\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[0;32m--> 982\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[1;32m    983\u001b[0m         name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[1;32m    984\u001b[0m       )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (2, 2) but got shape (2, 16) instead for parameter \"kernel\" in \"/scan(MetaOpt)/linear_in/layers_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "z, _ = apply_model({'params':param, 'state':state_init}, signal, truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiber2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
