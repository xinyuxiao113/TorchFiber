{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data number: 1\n",
      "Test Data number: 1\n"
     ]
    }
   ],
   "source": [
    "import os, time, torch, numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from src.TorchDSP.dataloader import MyDataset \n",
    "from src.TorchDSP.eq import eqAMPBC, eqPBC, eqPBC_step, MultiStepPBC, eqAMPBC_step\n",
    "from src.TorchDSP.loss import MSE, Qsq\n",
    "from src.TorchSimulation.receiver import BER\n",
    "\n",
    "\n",
    "\n",
    "window_size = 101\n",
    "TBPL = 100000\n",
    "Nwindow = 1\n",
    "train_data = MyDataset('dataset_A800/train.h5', Nch=[21], Rs=[80], Pch=[2], \n",
    "                       window_size=window_size + TBPL - 1, strides=TBPL, Nwindow=Nwindow, truncate=20000, \n",
    "                       Tx_window=True, pre_transform='Rx_CDCDDLMS(taps=32,lr=[0.015625, 0.0078125])')\n",
    "\n",
    "test_data = MyDataset('dataset_A800/test.h5', Nch=[21], Rs=[80], Pch=[2], \n",
    "                       window_size=window_size + TBPL - 1, strides=TBPL, Nwindow=Nwindow, truncate=20000, \n",
    "                       Tx_window=True, pre_transform='Rx_CDCDDLMS(taps=32,lr=[0.015625, 0.0078125])')\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "print('Train Data number:',len(train_data))\n",
    "print('Test Data number:',len(test_data))\n",
    "\n",
    "def Test(net, dataloader):\n",
    "    net.eval()\n",
    "    mse, power, ber, N = 0,0,0, len(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for Rx, Tx, info in dataloader:\n",
    "            Rx, Tx, info = Rx.cuda(), Tx.cuda(), info.cuda()\n",
    "            symb = Tx[:,net.overlaps//2:-net.overlaps//2]\n",
    "            PBC = net(Rx, info)\n",
    "            mse += MSE(PBC, symb).item()\n",
    "            power += MSE(symb, 0).item() \n",
    "            ber += np.mean(BER(PBC, symb)['BER'])\n",
    "    return {'MSE':mse/N, 'SNR': 10*np.log10(power/mse), 'BER':ber/N, 'Qsq': Qsq(ber/N)}\n",
    "\n",
    "def write_log(writer, epoch, train_loss, metric):\n",
    "    writer.add_scalar('Loss/train',  train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Test', metric['MSE'], epoch)\n",
    "    writer.add_scalar('Metric/SNR', metric['SNR'], epoch)\n",
    "    writer.add_scalar('Metric/BER', metric['BER'], epoch)\n",
    "    writer.add_scalar('Metric/Qsq', metric['Qsq'], epoch)\n",
    "    print(epoch, 'Train MSE:',  train_loss, 'Test MSE:', metric['MSE'],  'Qsq:', metric['Qsq'], flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per epoch:  2.0797057151794434\n",
      "0 Train MSE: 0.03528790920972824 Test MSE: 0.03243440389633179 Qsq: 7.9560107776447255\n",
      "Time per epoch:  0.1890885829925537\n",
      "1 Train MSE: 0.032690130174160004 Test MSE: 0.0324343740940094 Qsq: 7.955516123994853\n",
      "Time per epoch:  0.0969095230102539\n",
      "2 Train MSE: 0.032690130174160004 Test MSE: 0.0324343740940094 Qsq: 7.955516123994853\n",
      "Time per epoch:  0.09644460678100586\n",
      "3 Train MSE: 0.032690130174160004 Test MSE: 0.0324343740940094 Qsq: 7.955516123994853\n",
      "Time per epoch:  0.09630608558654785\n",
      "4 Train MSE: 0.032690130174160004 Test MSE: 0.0324343740940094 Qsq: 7.955516123994853\n",
      "Time per epoch:  0.09744429588317871\n",
      "5 Train MSE: 0.032690130174160004 Test MSE: 0.0324343740940094 Qsq: 7.955516123994853\n",
      "Time per epoch:  0.0966196060180664\n",
      "6 Train MSE: 0.032690130174160004 Test MSE: 0.0324343740940094 Qsq: 7.955516123994853\n",
      "Time per epoch:  0.09613180160522461\n",
      "7 Train MSE: 0.032690130174160004 Test MSE: 0.0324343740940094 Qsq: 7.955516123994853\n",
      "Time per epoch:  0.09659147262573242\n",
      "8 Train MSE: 0.032690130174160004 Test MSE: 0.0324343740940094 Qsq: 7.955516123994853\n",
      "Time per epoch:  0.09786462783813477\n",
      "9 Train MSE: 0.032690130174160004 Test MSE: 0.0324343740940094 Qsq: 7.955516123994853\n"
     ]
    }
   ],
   "source": [
    "log_path = '_outputs/log_test/PBC_step_LBFGS'\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "writer = SummaryWriter(log_path)\n",
    "epochs = 10\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=False, drop_last=True)\n",
    "data_iter = iter(train_loader)\n",
    "Rx,Tx,info = next(data_iter)\n",
    "Rx, Tx, info = Rx.cuda(), Tx.cuda(), info.cuda()\n",
    "\n",
    "# net = MultiStepPBC(2, window_size, index_type='reduce-2', pol_share=True)\n",
    "net = eqPBC_step(window_size, index_type='reduce-2', pol_share=True)\n",
    "# net = eqAMPBC_step(window_size, 1, fwm_share=True)\n",
    "net.cuda()\n",
    "optimizer = torch.optim.LBFGS(net.parameters(), lr=1)\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    PBC = net(Rx, info)\n",
    "    loss = MSE(PBC, Tx[:,net.overlaps//2:-net.overlaps//2])\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    t0 = time.time()\n",
    "    loss = optimizer.step(closure)\n",
    "    t1 = time.time() \n",
    "    print('Time per epoch: ', t1 - t0, flush=True)\n",
    "    write_log(writer, epoch, loss.item(), Test(net, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('_outputs/log_test/PBC_step_adam', exist_ok=True)\n",
    "writer = SummaryWriter('_outputs/log_test/PBC_step_adam')\n",
    "epochs = 30\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=10000, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=10000, shuffle=True, drop_last=True)\n",
    "\n",
    "net = eqAMPBC(M=window_size, rho=1)\n",
    "net.cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for i, (Rx, Tx, info) in enumerate(train_loader):\n",
    "        Rx, Tx, info = Rx.cuda(), Tx.cuda(), info.cuda()\n",
    "        y = net(Rx)\n",
    "        loss = MSE(y, Tx)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  \n",
    "        writer.add_scalar('Loss/train_batch', loss.item(), epoch*len(train_loader)+i)\n",
    "    scheduler.step()\n",
    "    metric = Test(net, test_loader)\n",
    "    write_log(writer, epoch, train_loss/len(train_loader), metric)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiber2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
