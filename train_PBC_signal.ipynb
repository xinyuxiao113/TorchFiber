{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data number: 100000\n",
      "Test Data number: 80000\n"
     ]
    }
   ],
   "source": [
    "import os, time, torch, numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from src.TorchDSP.dataloader import MyDataset \n",
    "from src.TorchDSP.nneq import eqAMPBC\n",
    "from src.TorchDSP.pbc import AmFoPBC, TorchSignal, TorchTime\n",
    "from src.TorchDSP.loss import MSE, Qsq\n",
    "from src.TorchSimulation.receiver import BER\n",
    "\n",
    "\n",
    "\n",
    "window_size = 401\n",
    "Nwindow = 100000\n",
    "TBPL = 10000\n",
    "train_data = MyDataset('dataset_A800/train.h5', Nch=[21], Rs=[80], Pch=[2], \n",
    "                       window_size=window_size, strides=1, Nwindow=Nwindow, truncate=20000, \n",
    "                       Tx_window=True, pre_transform='Rx_CDCDDLMS(taps=32,lr=[0.015625, 0.0078125])')\n",
    "\n",
    "test_data = MyDataset('dataset_A800/test.h5', Nch=[21], Rs=[80], Pch=[2], \n",
    "                       window_size=window_size, strides=1, Nwindow=80000, truncate=20000, \n",
    "                       Tx_window=True, pre_transform='Rx_CDCDDLMS(taps=32,lr=[0.015625, 0.0078125])')\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=10000, shuffle=True, drop_last=True)\n",
    "\n",
    "print('Train Data number:',len(train_data))\n",
    "print('Test Data number:',len(test_data))\n",
    "\n",
    "def Test(net, dataloader):\n",
    "    net.eval()\n",
    "    mse, power, ber, N = 0,0,0, len(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for Rx, Tx, info in dataloader:\n",
    "            Rx, Tx, info = Rx.cuda(), Tx.cuda(), info.cuda()\n",
    "            Rx = TorchSignal(val=Rx, t=TorchTime(0,0,1))\n",
    "            y = net(Rx, info)\n",
    "            symb = Tx[:,y.t.start:y.t.stop]\n",
    "            loss = MSE(y.val, symb)\n",
    "            mse += loss.item()\n",
    "            power += MSE(symb, 0).item() \n",
    "            ber += np.mean(BER(y.val.view(-1, y.val.shape[-1]), symb.view(-1, symb.shape[-1]))['BER'])\n",
    "    return {'MSE':mse/N, 'SNR': 10*np.log10(power/mse), 'BER':ber/N, 'Qsq': Qsq(ber/N)}\n",
    "\n",
    "def write_log(writer, epoch, train_loss, metric):\n",
    "    writer.add_scalar('Loss/train',  train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Test', metric['MSE'], epoch)\n",
    "    writer.add_scalar('Metric/SNR', metric['SNR'], epoch)\n",
    "    writer.add_scalar('Metric/BER', metric['BER'], epoch)\n",
    "    writer.add_scalar('Metric/Qsq', metric['Qsq'], epoch)\n",
    "    print(epoch, 'Train MSE:',  train_loss, 'Test MSE:', metric['MSE'],  'Qsq:', metric['Qsq'], flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. AMFoPBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 31.19 MiB is free. Including non-PyTorch memory, this process has 15.73 GiB memory in use. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 693.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m Rx, Tx, info \u001b[38;5;241m=\u001b[39m Rx\u001b[38;5;241m.\u001b[39mcuda(), Tx\u001b[38;5;241m.\u001b[39mcuda(), info\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     17\u001b[0m Rx \u001b[38;5;241m=\u001b[39m TorchSignal(val\u001b[38;5;241m=\u001b[39mRx, t\u001b[38;5;241m=\u001b[39mTorchTime(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 18\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m MSE(y\u001b[38;5;241m.\u001b[39mval, Tx[:,y\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m.\u001b[39mstart:y\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m.\u001b[39mstop])\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniforge3/envs/fiber2024/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/fiber2024/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/TorchFiber/src/TorchDSP/pbc.py:729\u001b[0m, in \u001b[0;36mAmFoPBC.forward\u001b[0;34m(self, signal, task_info)\u001b[0m\n\u001b[1;32m    726\u001b[0m Nmodes \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mval\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# IFWM term\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonlinear_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m                         \u001b[38;5;66;03m# [batch, W, Nmodes, len(S)] or [W, Nmodes, len(S)]\u001b[39;00m\n\u001b[1;32m    730\u001b[0m features \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverlaps\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m):\u001b[38;5;241m-\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverlaps\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m),:,:]   \u001b[38;5;66;03m# [batch, W-L, Nmodes, len(S)] or [W-L, Nmodes, len(S)]\u001b[39;00m\n\u001b[1;32m    731\u001b[0m E1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn1(features[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m*\u001b[39m P[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m])                     \u001b[38;5;66;03m# [batch, W-L, 1] or [W-L, 1]\u001b[39;00m\n",
      "File \u001b[0;32m~/TorchFiber/src/TorchDSP/pbc.py:458\u001b[0m, in \u001b[0;36mSymPBC.nonlinear_features\u001b[0;34m(self, E)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(m,n) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mabs\u001b[39m(m):\n\u001b[0;32m--> 458\u001b[0m         A \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    459\u001b[0m         Emn \u001b[38;5;241m=\u001b[39m (A \u001b[38;5;241m+\u001b[39m A\u001b[38;5;241m.\u001b[39mroll(\u001b[38;5;241m1\u001b[39m, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mroll(E, m, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    460\u001b[0m         A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mroll(E, m, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mroll(E, m \u001b[38;5;241m+\u001b[39m n, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mconj() \n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 31.19 MiB is free. Including non-PyTorch memory, this process has 15.73 GiB memory in use. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 693.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "os.makedirs('_outputs/log_test/AMFoPBC_Adam', exist_ok=True)\n",
    "writer = SummaryWriter('_outputs/log_test/AMFoPBC_Adam')\n",
    "epochs = 30\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=5000, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=5000, shuffle=True, drop_last=True)\n",
    "\n",
    "net = AmFoPBC(L=window_size - 1, rho=1)\n",
    "net.cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for i, (Rx, Tx, info) in enumerate(train_loader):\n",
    "        Rx, Tx, info = Rx.cuda(), Tx.cuda(), info.cuda()\n",
    "        Rx = TorchSignal(val=Rx, t=TorchTime(0,0,1))\n",
    "        y = net(Rx, info)\n",
    "        loss = MSE(y.val, Tx[:,y.t.start:y.t.stop])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()  \n",
    "        writer.add_scalar('Loss/train_batch', loss.item(), epoch*len(train_loader)+i)\n",
    "    scheduler.step()\n",
    "    metric = Test(net, test_loader)\n",
    "    write_log(writer, epoch, train_loss/len(train_loader), metric)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiber2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
